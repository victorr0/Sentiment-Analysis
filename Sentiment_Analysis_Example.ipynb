{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "First conceptual modelling with scraped review data. The goal is to analyse the sentiment of a review and correctly predict that sentiment.\n",
    "\n",
    "After letting Scrapy do it's work I got a dataset with 50k reviews. All of these have been labeled already.\n",
    "\n",
    "I'll process the reviews file here before building a model to classify the sentiment. In this notebook I'll just use 2 classes for simplicity, the \"positive\"- and \"negative\"-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST. (),!?\\'\\`\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9ëéèáé]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def getstats(format_pos_sampled,format_neg_sampled):\n",
    "    poslen = [len(i) for i in format_pos_sampled]\n",
    "    neglen = [len(i) for i in format_neg_sampled]\n",
    "    print('\\n \\t\\tpos\\t\\tneg')\n",
    "    print(\"count :\",'\\t',int(len(format_pos_sampled)),'\\t',int(len(format_neg_sampled)))\n",
    "    print(\"mean  :\",'\\t',int(statistics.mean(poslen)),'\\t\\t',int(statistics.mean(neglen)))\n",
    "    print('median:','\\t',int(statistics.median(poslen)),'\\t\\t',int(statistics.median(neglen)))\n",
    "    print('stdev :','\\t',int(statistics.stdev(poslen)),'\\t\\t',int(statistics.stdev(neglen)))\n",
    "    return(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "datasetneg = []\n",
    "datasetpos = []\n",
    "with open('data/review_text_labeled_bare.csv', 'r', encoding='utf8') as reviews:\n",
    "    for review in reviews:\n",
    "        label = review.split()[0]\n",
    "        if label == 'positive':\n",
    "            datasetpos.append(clean_str(str(review)))\n",
    "        elif label == 'negative':\n",
    "            datasetneg.append(clean_str(str(review)))\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Here are 2 positive examples:\")\n",
    "print(datasetpos[123])\n",
    "print(datasetpos[456],'\\n')\n",
    "print(\"Here are 2 negative examples:\")\n",
    "print(datasetneg[123])\n",
    "print(datasetneg[456])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model will train with this compiled dataset, to ultimately be able to classify unknown new reviews into either positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "format_neg_sampled = [i for i in datasetneg if len(i) >= 150 and len(i) <= 500]\n",
    "format_pos_sampled = [i for i in datasetpos if len(i) >= 150 and len(i) <= 500]\n",
    "print(\"Statistics for the whole dataset:\")\n",
    "print(getstats(datasetpos,datasetneg))\n",
    "print()\n",
    "print(\"Statistics for the reviews between 150 and 500 words long:\")\n",
    "print(getstats(format_pos_sampled,format_neg_sampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'm taking 5000 samples of each label. Then I pop a negative and positive from each sampled set to create the new final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pos_sampled = random.sample(format_pos_sampled, k=5000)\n",
    "neg_sampled = random.sample(format_neg_sampled, k=5000)\n",
    "\n",
    "# This is the 'final' dataset our deep learning model will use\n",
    "dataset = []\n",
    "for i in range(10000):\n",
    "    if i%2 == 0:\n",
    "        dataset.append(pos_sampled.pop())\n",
    "    elif i% 2 != 0:\n",
    "        dataset.append(neg_sampled.pop())\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# And saving the dataset to CSV\n",
    "with open('reviews_labels.csv', 'w',encoding='utf-8') as file:\n",
    "    file.writelines(\"\\n\".join([i for i in dataset]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note: In the terminal I used:<br>\n",
    "<code> cat reviews_label.csv | sed -e 's/positive /positive;/g' | sed -e 's/negative /negative;/g' > dataset.csv </code><br>\n",
    "to make it easier for Pandas to read. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Finally, modelling\n",
    "Now that preprocessing is done it's time to start modelling. I'll load the just saved csv into a Pandas DataFrame and create a Series for the reviews and a series for the Labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Now creating Pandas Dataframe from cvs\")\n",
    "reviewsall = pd.read_csv('review_dataset.csv', sep=';', header=None)\n",
    "\n",
    "# Pandas Series for the reviews\n",
    "reviews = reviewsall[1]\n",
    "\n",
    "# Pandas Series for the labels for each of those series\n",
    "labels = reviewsall[0]\n",
    "\n",
    "print(\"Finished loading Reviews and Labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Counting frequencies\n",
    "To create a CBOW/Continues Bag of Words I'll need to count how often each word appears in the data. I'll use these frequencies to create a vocabulary for encoding the review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "total_counts = Counter()\n",
    "for _, row in reviews.iteritems():\n",
    "    total_counts.update(row.split(' '))\n",
    "print(\"Total words in dataset: \", len(total_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Vocab & Word2IDx\n",
    "----\n",
    "I'll initially use the first 20000 words of my vocab for the model, this will be my vocab\n",
    "\n",
    "I'll also create a dictionary called word2idx, which maps each word in the vocab to an index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab = sorted(total_counts, key=total_counts.get, reverse=True)[:20000]\n",
    "\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Wordvectorization\n",
    "The following function takes a string and returns a numpy array of the words' vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    word_vector = np.zeros(len(vocab), dtype=np.int_)\n",
    "    for word in text.split(' '):\n",
    "        idx = word2idx.get(word, None)\n",
    "        if idx is None:\n",
    "            continue\n",
    "        else:\n",
    "            word_vector[idx] += 1\n",
    "    return np.array(word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I'll finally run through all of my final dataset and convert each review to a word vector with the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_vectors = np.zeros((len(reviews), len(vocab)), dtype=np.int_)\n",
    "for ii, (_, text) in enumerate(reviews.iteritems()):\n",
    "    word_vectors[ii] = text_to_vector(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train and Test Datasets\n",
    "To keep it simple I'll use TFLearn's function <code>to_categorical</code> to reshape the data into two output units which it can classify with a softmax activation function. I'm not worried about the validation set because TFLearn does this automatically :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y = (labels=='positive').astype(np.int_)\n",
    "records = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "shuffle = np.arange(records)\n",
    "np.random.shuffle(shuffle)\n",
    "test_fraction = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_split, test_split = shuffle[:int(records*test_fraction)], shuffle[int(records*test_fraction):]\n",
    "trainX, trainY = word_vectors[train_split,:], to_categorical(Y.values[train_split], 2)\n",
    "testX, testY = word_vectors[test_split,:], to_categorical(Y.values[test_split], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building our TrustPilot Net\n",
    "\n",
    "<b>Input Layer</b><br>\n",
    "input layer = amount of units, in my case I use 20000 element long vectors to encode -> 20000 input units\n",
    "\n",
    "<b>More (Hidden!) Layers</b><br>\n",
    "I add two hidden layers, each one adds a fully connected layer where every unit in the previous layer is connected to every unit in this layer.\n",
    "\n",
    "<b>Output Layer</b><br>\n",
    "The output layer is what we actually want to see. Since I've turned this sentiment analysis problem into a classification problem with two labels, the output layer will have size 2. The appropriate activation function for trying to predict if some input data belongs to one of two classes, I use softmax\n",
    "\n",
    "<b>Training</b><br>\n",
    "With TFLearn you use <code> fit </code> to train a model. A usage example is:<br>\n",
    "<code> net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy') </code><br>\n",
    "<br>\n",
    "Hyperparameters for the training command are the following:\n",
    "* `optimizer` sets the training method, here stochastic gradient descent\n",
    "* `learning_rate` = learning rate\n",
    "* `loss` Network error calculated from with the categorical cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Input Layer\n",
    "    net = tflearn.input_data([None, 20000])\n",
    "\n",
    "    # More Layers (hidden)\n",
    "    net = tflearn.fully_connected(net, 200, activation='ReLU')\n",
    "    net = tflearn.fully_connected(net, 25, activation='ReLU')\n",
    "    \n",
    "    # Output layer > one of 2 classes\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='sgd',learning_rate=0.01,loss='categorical_crossentropy')\n",
    "    \n",
    "    model = tflearn.DNN(net)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialise the model (exciting!)\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Training, this is where the action starts! \n",
    "# Action which unfortunately consists mostly of waiting.\n",
    "# First 10 epochs with batch size 256\n",
    "model.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=265, n_epoch=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# That did not increase accuray, nor did it drop the loss. Now 20 epochs with batch size 128\n",
    "model.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=128, n_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Wow, increased accuray, but loss is still high... Let's try 10 epochs with batch size 64\n",
    "model.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=64, n_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Testing with our Test Dataset\n",
    "Note: I ran model.fit 30 epochs with batch size 128 first. The accuracy was high (95%), but I was not satisfied with the decrease in loss.\n",
    "\n",
    "That's why I rand model.fit 20 more epochs with batch size 256. This decreased the total loss.\n",
    "Now that the model has been trained with the Train Dataset, it has extracted features to classify a review into 1 of 2 classes, either positive or negative. \n",
    "\n",
    "To put the accuracy the test I'll let the model make a prediction for each of the Test Dataset's reviews. I'll then compare these predictions to the ground truth from the dataset itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions = (np.array(model.predict(testX))[:,0] >= 0.5).astype(np.int_)\n",
    "testdataset_accuracy = np.mean(predictions == testY[:,0], axis=0)\n",
    "print(\"Test Dataset accuracy: \", testdataset_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>94% Accuracy</b> is not bad :), for a first draft model, <i>if I may pat myself on the back prematurely</i>. \n",
    "\n",
    "But how does the model work with sentences it hasn't seen before? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Testing with trivial examples\n",
    "--\n",
    "Here I'll test the model's output with some trivial example sentences in Dutch. The model figures this out by looking at the features learned from the examples given in training. The probability calculation works as follows;\n",
    "\n",
    "The model first predicts the probability of the sentence's vector, meaning what's the chance that given the sentence's tokens in that order, transformed into a vector, would correspond to one of the two labels. \n",
    "\n",
    "The function <code> test_sentence(zin) </code> below calculates the probability of a sentence (zin) being positive. If the probability is high (>0.5), the chance of the prediction being correct/accurate is high as well.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_sentence(zin):\n",
    "    positive_prob = model.predict([text_to_vector(zin.lower())])[0][1]\n",
    "    print('Sentence: {}'.format(zin))\n",
    "    print('P(positive) = {:.3f} :'.format(positive_prob), \n",
    "          'Positive' if positive_prob > 0.5 else 'Negative')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "zin = \"Transformers is de zeker weten de beste film van 2016\"\n",
    "test_sentence(zin)\n",
    "print()\n",
    "\n",
    "zin = \"Transformers is de niet de beste film van 2016\"\n",
    "test_sentence(zin)\n",
    "print()\n",
    "\n",
    "zin = \"Het is ongelofelijk hoe iemand met talent iets zo spectaculair lelijk maakt\"\n",
    "test_sentence(zin)\n",
    "print()\n",
    "\n",
    "zin = \"Het is ongelofelijk dat iemand van deze vieze rommel houdt.\"\n",
    "test_sentence(zin)\n",
    "print()\n",
    "\n",
    "zin = \"Het is niet te geloven dat iemand zoiets prachtigs vieze rommel noemt.\"\n",
    "test_sentence(zin)\n",
    "print()\n",
    "\n",
    "zin = \"Ik vind het product erg goed en het ziet er ook nog mooi uit. Aan te raden. Wel kopen.\"\n",
    "test_sentence(zin)\n",
    "print()\n",
    "\n",
    "zin = \"Ik vind het product erg slecht en het ziet er ook nog lelijk uit. Af te raden. Niet kopen.\"\n",
    "test_sentence(zin)\n",
    "print()\n",
    "\n",
    "zin = \"Ik raad dit bedrijf af, zeer slechte service en beroerd behandeld. Helemaal niet tevreden\"\n",
    "test_sentence(zin)\n",
    "print()\n",
    "\n",
    "zin = \"Ik raad dit bedrijf aan, zeer goede service en keurig behandeld. Helemaal tevreden\"\n",
    "test_sentence(zin)\n",
    "print()\n",
    "\n",
    "zin = \"Bij dit bedrijf ben ik vreselijk slecht te woord gestaan. Ik kwam voor een reparatie maar dat is niet goed gegaan. Uiteindelijk heeft het me handen vol geld gekost en ben ik niet opgeschoten. Rotbedrijf\"\n",
    "test_sentence(zin)\n",
    "print()\n",
    "\n",
    "zin = \"Bij dit bedrijf ben ik fantastisch goed te woord gestaan. Ik kwam voor een reparatie en dat is helemaal goed gegaan. Uiteindelijk heeft het me weinig geld gekost en ben ik geholpen. Topbedrijf\"\n",
    "test_sentence(zin)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
